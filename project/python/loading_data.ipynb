{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bigquery\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse arguments\n",
    "parser = argparse.ArgumentParser(description='GCP setup')\n",
    "parser.add_argument('--projectid', required=True, help='GCP Project ID')\n",
    "parser.add_argument('--BIGQuerydataset', required=True, help='BigQuery Dataset Name')\n",
    "parser.add_argument('--bucket', required=True, help='Bucket Name')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# jar for spark run in GCP\n",
    "bucket_jar = f\"gs://{args.bucket}/jars\"\n",
    "bigquery_jar = \"spark-3.5-bigquery-0.42.1.jar\"\n",
    "gcs_connector_jar = \"gcs-connector-hadoop3-latest.jar\"\n",
    "\n",
    "# variables\n",
    "GCP_projectID = args.projectid\n",
    "BigQuery_dataset = args.BIGQuerydataset\n",
    "bucket = args.bucket\n",
    "\n",
    "\n",
    "# buckets link\n",
    "birthRate_gs = f\"gs://{args.bucket}/birth-rate_*.csv\"\n",
    "life_gs = f\"gs://{args.bucket}/life-expectancy_*.csv\"\n",
    "refugee_gs = f\"gs://{args.bucket}/refugee-population_*.csv\"\n",
    "migrant_gs = f\"gs://{args.bucket}/migrant-total_*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DE_Zoomcamp_Population\") \\\n",
    "    .config(\"spark.jars\", f\"{bucket_jar}/{bigquery_jar},{bucket_jar}/{gcs_connector_jar}\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# read data from bucket\n",
    "birth_rate_df = spark.read.csv(birthRate_gs) \\\n",
    "                        .option(delimiter=\",\", header=True)\n",
    "life_df = spark.read.csv(life_gs) \\\n",
    "                        .option(delimiter=\",\", header=True)\n",
    "refugee_df = spark.read.csv(refugee_gs) \\\n",
    "                        .option(delimiter=\",\", header=True)\n",
    "migrant_df = spark.read.csv(migrant_gs) \\\n",
    "                        .option(delimiter=\",\", header=True)\n",
    "\n",
    "\n",
    "# create dim table\n",
    "country_df = life_df.groupBy([\"Code\", \"Entity\"]).count()\\\n",
    "              .drop(\"count\")\n",
    "\n",
    "# dropping country name columns for all the tables\n",
    "birth_rate_clean  =   birth_rate_df.drop(\"Entity\")\n",
    "life_clean        =   life_df.drop(\"Entity\")\n",
    "refugee_clean     =   refugee_df.drop(\"Entity\")\n",
    "migrant_clean     =   migrant_df.drop(\"Entity\")\n",
    "\n",
    "\n",
    "# merging four tables together\n",
    "combined_df = birth_rate_clean \\\n",
    "              .join(life_clean,     on=[\"Code\", \"Year\"], how= \"left\") \\\n",
    "              .join(refugee_clean,  on=[\"Code\", \"Year\"], how= \"left\") \\\n",
    "              .join(migrant_clean,  on=[\"Code\", \"Year\"], how= \"left\" )\n",
    "\n",
    "# filter out no country code and then replace null to 0\n",
    "combined_df = combined_df.filter(combined_df.Code.isNotNull()) \n",
    "                         \n",
    "              \n",
    "# rename column\n",
    "combined_df = combined_df \\\n",
    "              .withColumnRenamed(\"Birth rate (historical)\", \"birth_rate\") \\\n",
    "              .withColumnRenamed(\"Life expectancy - Sex: total - Age: 0 - Type: period\", \"life_expectancy\") \\\n",
    "              .withColumnRenamed(\"Total number of international immigrants\", \"international_immigrants\") \\\n",
    "              .withColumnRenamed(\"Refugees by country of origin\", \"refugees\")\n",
    "\n",
    "# changing column type\n",
    "combined_df = combined_df \\\n",
    "              .withColumn(\"Year\",                     F.col(\"Year\").cast(\"int\")) \\\n",
    "              .withColumn(\"birth_rate\",               F.col(\"birth_rate\").cast(\"float\")) \\\n",
    "              .withColumn(\"life_expectancy\",          F.col(\"life_expectancy\").cast(\"float\")) \\\n",
    "              .withColumn(\"international_immigrants\", F.col(\"international_immigrants\").cast(\"float\")) \\\n",
    "              .withColumn(\"refugees\",                 F.col(\"refugees\").cast(\"float\")) \n",
    "\n",
    "# replace 0 for all the null values\n",
    "combined_df = combined_df.fillna(0, subset=[\"birth_rate\", \"life_expectancy\", \"international_immigrants\", \"refugees\"])\n",
    "\n",
    "\n",
    "# BigQuery tables\n",
    "master_table = \"fact_population\"\n",
    "country = \"dim_country\"\n",
    "\n",
    "# writing tables with partition and clustering the table\n",
    "combined_df.write.format(\"bigquery\").mode(\"append\") \\\n",
    "            .option(\"writeMethod\", \"direct\") \\\n",
    "            .saveAsTable(f\"{GCP_projectID}:{BigQuery_dataset}.{master_table}\")\n",
    "\n",
    "country_df.write.format(\"bigquery\").mode(\"append\") \\\n",
    "            .option(\"writeMethod\", \"direct\") \\\n",
    "            .saveAsTable(f\"{GCP_projectID}:{BigQuery_dataset}.{country}\")\n",
    "\n",
    "\n",
    "\n",
    "# stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineerZoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
